{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f64b8bf9-5bcb-4c44-b901-307c8fdc9570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**SQL 50 Link: https://leetcode.com/studyplan/top-sql-50/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9734b5b-42c7-41f5-8f76-d9a80b1645db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8224caf7-e491-4056-8687-a68f56869c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"low_fats\", StringType(), True),\n",
    "    StructField(\"recyclable\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    (0, \"Y\", \"N\"),\n",
    "    (1, \"Y\", \"Y\"),\n",
    "    (2, \"N\", \"Y\"),\n",
    "    (3, \"Y\", \"Y\"),\n",
    "    (4, \"N\", \"N\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce8de5eb-72c7-4956-83d2-6447da8da3d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a727068b-c155-4bd9-b0dc-a5bd04bbbac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter((col(\"low_fats\") == \"Y\") & (col(\"recyclable\") == \"Y\")).select(\n",
    "    \"product_id\"\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b30f8cf-66ef-4b4a-bb5a-447374293742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05673002-0ebe-40e9-bc13-7caab4e280d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"referee_id\", IntegerType(), True) \n",
    "])\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    (1, \"Will\", None),\n",
    "    (2, \"Jane\", None),\n",
    "    (3, \"Alex\", 2),\n",
    "    (4, \"Bill\", None),\n",
    "    (5, \"Zack\", 1),\n",
    "    (6, \"Mark\", 2)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92251def-0de8-40a5-8bd1-9cdced6a1e4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c118c79-1f6f-40d2-97e0-07005e047b01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter((col('referee_id') != 2) | (col('referee_id').isNull())).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a287f27-a040-44f2-9008-721392436814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61fcac66-e3d2-4990-a518-64d69f05384c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"continent\", StringType(), True),\n",
    "    StructField(\"area\", LongType(), True),\n",
    "    StructField(\"population\", LongType(), True),\n",
    "    StructField(\"gdp\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    (\"Afghanistan\", \"Asia\", 652230, 25500100, 20343000000),\n",
    "    (\"Albania\", \"Europe\", 28748, 2831741, 12960000000),\n",
    "    (\"Algeria\", \"Africa\", 2381741, 37100000, 188681000000),\n",
    "    (\"Andorra\", \"Europe\", 468, 78115, 3712000000),\n",
    "    (\"Angola\", \"Africa\", 1246700, 20609294, 100990000000)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15ccaad8-f27e-4ff4-9c90-73529c59e573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d400aab-b108-4dfc-9c76-77e487332f18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter((col('area')>=3000000) | (col('population')>=25000000)).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "156ad50e-01bc-47e4-995f-1f64b11fb796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98ef0575-2093-4ef4-be14-db41d91c392b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DateType\n",
    "from datetime import datetime\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"article_id\", IntegerType(), True),\n",
    "    StructField(\"author_id\", IntegerType(), True),\n",
    "    StructField(\"viewer_id\", IntegerType(), True),\n",
    "    StructField(\"view_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    (1, 3, 5, datetime.strptime(\"2019-08-01\", \"%Y-%m-%d\").date()),\n",
    "    (1, 3, 6, datetime.strptime(\"2019-08-02\", \"%Y-%m-%d\").date()),\n",
    "    (2, 7, 7, datetime.strptime(\"2019-08-01\", \"%Y-%m-%d\").date()),\n",
    "    (2, 7, 6, datetime.strptime(\"2019-08-02\", \"%Y-%m-%d\").date()),\n",
    "    (4, 7, 1, datetime.strptime(\"2019-07-22\", \"%Y-%m-%d\").date()),\n",
    "    (3, 4, 4, datetime.strptime(\"2019-07-21\", \"%Y-%m-%d\").date()),\n",
    "    (3, 4, 4, datetime.strptime(\"2019-07-21\", \"%Y-%m-%d\").date())\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show DataFrame\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5edb62d-15be-4f39-94e9-467346bd7927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7b9a36-8a59-4750-99bb-c57871bb991a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.filter((col('author_id') == col('viewer_id'))).select(col('author_id').alias(\"id\")).distinct().orderBy(asc(\"id\")).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71c61fd7-f96c-4185-827e-bb3ad364de3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70acbd7c-a143-4e7d-bf05-8b6a63ce1429",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"tweet_id\", IntegerType(), True),\n",
    "    StructField(\"content\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    (1, \"Let us Code\"),\n",
    "    (2, \"More than fifteen chars are here!\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73d995eb-410e-4bd6-a9d3-32489e41245f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "457d9de5-0bfc-4b65-a91d-a602c0db5676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length\n",
    "df.filter((length(col('content'))>15)).select(\"tweet_id\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c01cf1a9-4c06-4077-8c8e-6382038e3c43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66ef3524-0e2e-4fdd-906d-b8c2191c7efa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# --- Employees Table ---\n",
    "\n",
    "# Define schema for Employees\n",
    "employees_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Employees data\n",
    "employees_data = [\n",
    "    (1, \"Alice\"),\n",
    "    (7, \"Bob\"),\n",
    "    (11, \"Meir\"),\n",
    "    (90, \"Winston\"),\n",
    "    (3, \"Jonathan\")\n",
    "]\n",
    "\n",
    "# Create Employees DataFrame\n",
    "employees_df = spark.createDataFrame(employees_data, employees_schema)\n",
    "\n",
    "# --- EmployeeUNI Table ---\n",
    "\n",
    "# Define schema for EmployeeUNI\n",
    "employee_uni_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"unique_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# EmployeeUNI data\n",
    "employee_uni_data = [\n",
    "    (3, 1),\n",
    "    (11, 2),\n",
    "    (90, 3)\n",
    "]\n",
    "\n",
    "# Create EmployeeUNI DataFrame\n",
    "employee_uni_df = spark.createDataFrame(employee_uni_data, employee_uni_schema)\n",
    "\n",
    "# Show both DataFrames\n",
    "print(\"Employees DataFrame:\")\n",
    "employees_df.show()\n",
    "\n",
    "print(\"EmployeeUNI DataFrame:\")\n",
    "employee_uni_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d88b05a0-df15-4448-9c7e-ce814f2f9ab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39fe26b3-1714-4a14-9a3e-32ebc68df1bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *  #a\n",
    "df= employees_df.alias('e').join(employee_uni_df.alias(\"eu\"),col('e.id')==col('eu.id'),\"left\").select(\"eu.unique_id\",\"e.name\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98aabf67-487b-4a59-8292-e9b57ecfec90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de581601-229a-4265-a699-3e4e82ee3604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# --- Sales Table ---\n",
    "\n",
    "# Define schema for Sales\n",
    "sales_schema = StructType([\n",
    "    StructField(\"sale_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sales data\n",
    "sales_data = [\n",
    "    (1, 100, 2008, 10, 5000),\n",
    "    (2, 100, 2009, 12, 5000),\n",
    "    (7, 200, 2011, 15, 9000)\n",
    "]\n",
    "\n",
    "# Create Sales DataFrame\n",
    "sales_df = spark.createDataFrame(sales_data, sales_schema)\n",
    "\n",
    "# --- Product Table ---\n",
    "\n",
    "# Define schema for Product\n",
    "product_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"product_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Product data\n",
    "product_data = [\n",
    "    (100, \"Nokia\"),\n",
    "    (200, \"Apple\"),\n",
    "    (300, \"Samsung\")\n",
    "]\n",
    "\n",
    "# Create Product DataFrame\n",
    "product_df = spark.createDataFrame(product_data, product_schema)\n",
    "\n",
    "# Show both DataFrames\n",
    "print(\"Sales DataFrame:\")\n",
    "sales_df.show()\n",
    "\n",
    "print(\"Product DataFrame:\")\n",
    "product_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c5bc74b-bd0f-4af7-89e4-1c23f030fe5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9c5ff57-66db-4360-9421-1bb0342e799d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_df.alias(\"s\").join(product_df.alias(\"p\"),col('s.product_id') == col('p.product_id'),\"inner\").select(\"p.product_name\",\"s.year\",\"s.price\").display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0fbf5f0-3d21-4e6c-ba42-b76cf19aa118",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "112072df-53e1-4562-ae2a-c553caa7e6b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "\n",
    "# --- Visits Table ---\n",
    "\n",
    "# Define schema for Visits\n",
    "visits_schema = StructType([\n",
    "    StructField(\"visit_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Visits data\n",
    "visits_data = [\n",
    "    (1, 23),\n",
    "    (2, 9),\n",
    "    (4, 30),\n",
    "    (5, 54),\n",
    "    (6, 96),\n",
    "    (7, 54),\n",
    "    (8, 54)\n",
    "]\n",
    "\n",
    "# Create Visits DataFrame\n",
    "visits_df = spark.createDataFrame(visits_data, visits_schema)\n",
    "\n",
    "# --- Transactions Table ---\n",
    "\n",
    "# Define schema for Transactions\n",
    "transactions_schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), True),\n",
    "    StructField(\"visit_id\", IntegerType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Transactions data\n",
    "transactions_data = [\n",
    "    (2, 5, 310),\n",
    "    (3, 5, 300),\n",
    "    (9, 5, 200),\n",
    "    (12, 1, 910),\n",
    "    (13, 2, 970)\n",
    "]\n",
    "\n",
    "# Create Transactions DataFrame\n",
    "transactions_df = spark.createDataFrame(transactions_data, transactions_schema)\n",
    "\n",
    "# Show both DataFrames\n",
    "print(\"Visits DataFrame:\")\n",
    "visits_df.show()\n",
    "\n",
    "print(\"Transactions DataFrame:\")\n",
    "transactions_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4c4fe7c-de84-4673-9126-51660ba766ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "385dda6e-cafb-4f80-b65c-fedc194384dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "visits_df.alias(\"v\").join(\n",
    "    transactions_df.alias(\"t\"), col('v.visit_id') == col('t.visit_id'), \"left\"\n",
    ").filter(col(\"t.transaction_id\").isNull()).groupBy(col('v.customer_id')).agg(count(\"*\").alias(\"count_no_trans\")).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81edd879-ff7d-4c92-90ca-aafd0a9989ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b62b5df5-7846-452a-befe-ad68f2e5b247",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DateType\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Define schema for Weather\n",
    "weather_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"recordDate\", DateType(), True),\n",
    "    StructField(\"temperature\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Weather data\n",
    "weather_data = [\n",
    "    (1, datetime.strptime(\"2015-01-01\", \"%Y-%m-%d\").date(), 10),\n",
    "    (2, datetime.strptime(\"2015-01-02\", \"%Y-%m-%d\").date(), 25),\n",
    "    (3, datetime.strptime(\"2015-01-03\", \"%Y-%m-%d\").date(), 20),\n",
    "    (4, datetime.strptime(\"2015-01-04\", \"%Y-%m-%d\").date(), 30)\n",
    "]\n",
    "\n",
    "# Create Weather DataFrame\n",
    "weather_df = spark.createDataFrame(weather_data, weather_schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "weather_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eff4103b-51c9-4e05-9d83-36fc60fdbbab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1feac3c-f66c-4f7a-8424-bb9f55bf7706",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752765591938}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "myWindow = Window.orderBy(\"recordDate\")\n",
    "weather_df.withColumn(\"prev_day\", lag(col(\"recordDate\"), 1).over(myWindow)).withColumn(\n",
    "    \"prev_day_temp\", lag(col(\"temperature\"), 1).over(myWindow)\n",
    ").filter(\n",
    "    (abs(date_diff(col(\"recordDate\"), col(\"prev_day\"))) == 1)\n",
    "    & (col(\"temperature\") > col(\"prev_day_temp\"))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36fe22ba-fab0-475a-bc3d-f8ab13cb00b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ce9e6cd-92d8-4a6d-853b-30698001193d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "# Define schema for Activity\n",
    "activity_schema = StructType([\n",
    "    StructField(\"machine_id\", IntegerType(), True),\n",
    "    StructField(\"process_id\", IntegerType(), True),\n",
    "    StructField(\"activity_type\", StringType(), True),\n",
    "    StructField(\"timestamp\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Activity data\n",
    "activity_data = [\n",
    "    (0, 0, \"start\", 0.712),\n",
    "    (0, 0, \"end\", 1.520),\n",
    "    (0, 1, \"start\", 3.140),\n",
    "    (0, 1, \"end\", 4.120),\n",
    "    (1, 0, \"start\", 0.550),\n",
    "    (1, 0, \"end\", 1.550),\n",
    "    (1, 1, \"start\", 0.430),\n",
    "    (1, 1, \"end\", 1.420),\n",
    "    (2, 0, \"start\", 4.100),\n",
    "    (2, 0, \"end\", 4.512),\n",
    "    (2, 1, \"start\", 2.500),\n",
    "    (2, 1, \"end\", 5.000)\n",
    "]\n",
    "\n",
    "# Create Activity DataFrame\n",
    "activity_df = spark.createDataFrame(activity_data, activity_schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "activity_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10d8e1d7-0f54-4beb-9359-a42f8eedf2a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45c2adf9-bac1-4d67-9730-1b3b555b1e33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "activity_df.alias(\"a1\").join(\n",
    "    activity_df.alias(\"a2\"),\n",
    "    (\n",
    "        (col(\"a1.machine_id\") == col(\"a2.machine_id\"))\n",
    "        & (col(\"a1.process_id\") == col(\"a2.process_id\"))\n",
    "        & (col(\"a2.timestamp\") > col(\"a1.timestamp\"))\n",
    "    ),\n",
    "    \"inner\",\n",
    ").selectExpr(\n",
    "    \"a1.machine_id\", \"a1.process_id\", \"a2.timestamp - a1.timestamp as processing_time\"\n",
    ").groupBy(\"a1.machine_id\").agg((sum(col('processing_time'))/count(col('process_id'))).alias(\"avg_processing\")).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5774511-a022-4001-9d90-e30e32270622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 11**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c5717ef-f4e6-455e-aa0e-cfbfe66b5629",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# --- Employee Table ---\n",
    "\n",
    "# Define schema for Employee\n",
    "employee_schema = StructType([\n",
    "    StructField(\"empId\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"supervisor\", IntegerType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Employee data\n",
    "employee_data = [\n",
    "    (3, \"Brad\", None, 4000),\n",
    "    (1, \"John\", 3, 1000),\n",
    "    (2, \"Dan\", 3, 2000),\n",
    "    (4, \"Thomas\", 3, 4000)\n",
    "]\n",
    "\n",
    "# Create Employee DataFrame\n",
    "employee_df = spark.createDataFrame(employee_data, employee_schema)\n",
    "\n",
    "# --- Bonus Table ---\n",
    "\n",
    "# Define schema for Bonus\n",
    "bonus_schema = StructType([\n",
    "    StructField(\"empId\", IntegerType(), True),\n",
    "    StructField(\"bonus\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Bonus data\n",
    "bonus_data = [\n",
    "    (2, 500),\n",
    "    (4, 2000)\n",
    "]\n",
    "\n",
    "# Create Bonus DataFrame\n",
    "bonus_df = spark.createDataFrame(bonus_data, bonus_schema)\n",
    "\n",
    "# Show both DataFrames\n",
    "print(\"Employee DataFrame:\")\n",
    "employee_df.show()\n",
    "\n",
    "print(\"Bonus DataFrame:\")\n",
    "bonus_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a53e00d-5f22-442e-bd70-e475c22876b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1701d7a0-9949-411e-b3c6-769e9eaec97d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employee_df.alias(\"e\").join(\n",
    "    bonus_df.alias(\"b\"), col(\"e.empId\") == col(\"b.empId\"), \"left\"\n",
    ").filter((col(\"b.bonus\") < 1000) | (col(\"b.bonus\").isNull())).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7b1f92a-1dd1-4a41-b99c-4875a7b0668a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 12**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22901418-fef3-4b2e-816f-dd651325bbd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# --- Students Table ---\n",
    "\n",
    "students_schema = StructType([\n",
    "    StructField(\"student_id\", IntegerType(), True),\n",
    "    StructField(\"student_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "students_data = [\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (13, \"John\"),\n",
    "    (6, \"Alex\")\n",
    "]\n",
    "\n",
    "students_df = spark.createDataFrame(students_data, students_schema)\n",
    "\n",
    "# --- Subjects Table ---\n",
    "\n",
    "subjects_schema = StructType([\n",
    "    StructField(\"subject_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "subjects_data = [\n",
    "    (\"Math\",),\n",
    "    (\"Physics\",),\n",
    "    (\"Programming\",)\n",
    "]\n",
    "\n",
    "subjects_df = spark.createDataFrame(subjects_data, subjects_schema)\n",
    "\n",
    "# --- Examinations Table ---\n",
    "\n",
    "examinations_schema = StructType([\n",
    "    StructField(\"student_id\", IntegerType(), True),\n",
    "    StructField(\"subject_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "examinations_data = [\n",
    "    (1, \"Math\"),\n",
    "    (1, \"Physics\"),\n",
    "    (1, \"Programming\"),\n",
    "    (2, \"Programming\"),\n",
    "    (1, \"Physics\"),\n",
    "    (1, \"Math\"),\n",
    "    (13, \"Math\"),\n",
    "    (13, \"Programming\"),\n",
    "    (13, \"Physics\"),\n",
    "    (2, \"Math\"),\n",
    "    (1, \"Math\")\n",
    "]\n",
    "\n",
    "examinations_df = spark.createDataFrame(examinations_data, examinations_schema)\n",
    "\n",
    "# Show all DataFrames\n",
    "print(\"Students DataFrame:\")\n",
    "students_df.show()\n",
    "\n",
    "print(\"Subjects DataFrame:\")\n",
    "subjects_df.show()\n",
    "\n",
    "print(\"Examinations DataFrame:\")\n",
    "examinations_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "330783d4-2a23-4942-b614-776160e17735",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count,nullif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b70537b-a2f0-40ef-8c31-7d33488c7d83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "students_df.alias(\"s\").join(subjects_df.alias(\"su\"), how=\"cross\").join(\n",
    "    examinations_df.alias(\"e\"),\n",
    "    (\n",
    "        (col(\"s.student_id\") == col(\"e.student_id\"))\n",
    "        & (col(\"su.subject_name\") == col(\"e.subject_name\"))\n",
    "    ),\n",
    "    \"left\",\n",
    ").groupBy(\"s.student_id\", \"s.student_name\", \"su.subject_name\").agg(\n",
    "    count(\"e.subject_name\").alias(\"attended_exams\")\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e9ab735-ecd5-417f-9a37-62e7fbc2f345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 13**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b70a6628-bf13-4948-bf6f-ab2129b83ad7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "\n",
    "# Define schema for Employee\n",
    "employee_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"department\", StringType(), True),\n",
    "    StructField(\"managerId\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Employee data\n",
    "employee_data = [\n",
    "    (101, \"John\", \"A\", None),\n",
    "    (102, \"Dan\", \"A\", 101),\n",
    "    (103, \"James\", \"A\", 101),\n",
    "    (104, \"Amy\", \"A\", 101),\n",
    "    (105, \"Anne\", \"A\", 101),\n",
    "    (106, \"Ron\", \"B\", 101)\n",
    "]\n",
    "\n",
    "# Create Employee DataFrame\n",
    "employee_df = spark.createDataFrame(employee_data, employee_schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "employee_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acaea133-df7d-4184-a50c-e26110327a86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "employee_df.groupBy(\"managerId\").agg(count(\"id\").alias(\"num_direct_reportees\")).filter(\n",
    "    \"num_direct_reportees>=5\"\n",
    ").select(\"managerId\").alias(\"e1\").join(\n",
    "    employee_df.alias(\"e2\"), col(\"e1.managerId\") == col(\"e2.id\"), \"inner\"\n",
    ").select(\n",
    "    \"e2.name\"\n",
    ").alias(\n",
    "    \"name\"\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3064ed57-efd5-431e-89ca-0224c97292d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 14**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b3ba1c4-c69b-45a6-ab58-ea8425715f66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Signups Table ---\n",
    "\n",
    "signups_schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"time_stamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "signups_data = [\n",
    "    (3, datetime.strptime(\"2020-03-21 10:16:13\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "    (7, datetime.strptime(\"2020-01-04 13:57:59\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "    (2, datetime.strptime(\"2020-07-29 23:09:44\", \"%Y-%m-%d %H:%M:%S\")),\n",
    "    (6, datetime.strptime(\"2020-12-09 10:39:37\", \"%Y-%m-%d %H:%M:%S\"))\n",
    "]\n",
    "\n",
    "signups_df = spark.createDataFrame(signups_data, signups_schema)\n",
    "\n",
    "# --- Confirmations Table ---\n",
    "\n",
    "confirmations_schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"time_stamp\", TimestampType(), True),\n",
    "    StructField(\"action\", StringType(), True)\n",
    "])\n",
    "\n",
    "confirmations_data = [\n",
    "    (3, datetime.strptime(\"2021-01-06 03:30:46\", \"%Y-%m-%d %H:%M:%S\"), \"timeout\"),\n",
    "    (3, datetime.strptime(\"2021-07-14 14:00:00\", \"%Y-%m-%d %H:%M:%S\"), \"timeout\"),\n",
    "    (7, datetime.strptime(\"2021-06-12 11:57:29\", \"%Y-%m-%d %H:%M:%S\"), \"confirmed\"),\n",
    "    (7, datetime.strptime(\"2021-06-13 12:58:28\", \"%Y-%m-%d %H:%M:%S\"), \"confirmed\"),\n",
    "    (7, datetime.strptime(\"2021-06-14 13:59:27\", \"%Y-%m-%d %H:%M:%S\"), \"confirmed\"),\n",
    "    (2, datetime.strptime(\"2021-01-22 00:00:00\", \"%Y-%m-%d %H:%M:%S\"), \"confirmed\"),\n",
    "    (2, datetime.strptime(\"2021-02-28 23:59:59\", \"%Y-%m-%d %H:%M:%S\"), \"timeout\")\n",
    "]\n",
    "\n",
    "confirmations_df = spark.createDataFrame(confirmations_data, confirmations_schema)\n",
    "\n",
    "# Show DataFrames\n",
    "print(\"Signups DataFrame:\")\n",
    "signups_df.show(truncate=False)\n",
    "\n",
    "print(\"Confirmations DataFrame:\")\n",
    "confirmations_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b40298df-e577-4567-a4c6-9cb632af94ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = (\n",
    "    signups_df.alias(\"s\")\n",
    "    .join(confirmations_df.alias(\"c\"), (col(\"s.user_id\") == col(\"c.user_id\")), \"left\")\n",
    "    .select(\"s.*\", \"c.time_stamp\", \"c.action\")\n",
    ")\n",
    "\n",
    "df.groupBy(\"user_id\").agg(\n",
    "    sum(when((col(\"action\") == \"confirmed\"), 1.0).otherwise(0.0)).alias(\n",
    "        \"confirmed_count\"\n",
    "    ),\n",
    "    count(\"action\").alias(\"total_count\"),\n",
    ").withColumn(\n",
    "    \"confirmation_rate\",\n",
    "    when(\n",
    "        (col(\"total_count\") != 0),\n",
    "        round(\n",
    "            (col(\"confirmed_count\").cast(\"float\") / col(\"total_count\").cast(\"float\")), 2\n",
    "        ),\n",
    "    ).otherwise(0.0),\n",
    ").select(\n",
    "    \"user_id\", \"confirmation_rate\"\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07388f41-592a-476c-8dd1-d338cda2937c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 15**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b65a0b07-4911-4f52-9e68-e42116383883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "# Define schema\n",
    "cinema_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"movie\", StringType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"rating\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Define data\n",
    "cinema_data = [\n",
    "    (1, \"War\", \"great 3D\", 8.9),\n",
    "    (2, \"Science\", \"fiction\", 8.5),\n",
    "    (3, \"irish\", \"boring\", 6.2),\n",
    "    (4, \"Ice song\", \"Fantacy\", 8.6),\n",
    "    (5, \"House card\", \"Interesting\", 9.1)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "cinema_df = spark.createDataFrame(cinema_data, cinema_schema)\n",
    "\n",
    "# Show DataFrame\n",
    "cinema_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40113b3f-8397-478d-a4b9-45649d584ce3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cinema_df.filter((col(\"id\") % 2 != 0) & (col(\"description\") != \"boring\")).orderBy(\n",
    "    desc(\"rating\")\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48c3be09-36c9-49d8-bdd3-38d102baa417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 16**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f377ec55-531c-4873-ac51-23fc765bf9e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DateType\n",
    "\n",
    "# Define schema for Prices\n",
    "prices_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"start_date\", DateType(), True),\n",
    "    StructField(\"end_date\", DateType(), True),\n",
    "    StructField(\"price\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Define data for Prices\n",
    "prices_data = [\n",
    "    (1, \"2019-02-17\", \"2019-02-28\", 5),\n",
    "    (1, \"2019-03-01\", \"2019-03-22\", 20),\n",
    "    (2, \"2019-02-01\", \"2019-02-20\", 15),\n",
    "    (2, \"2019-02-21\", \"2019-03-31\", 30)\n",
    "]\n",
    "\n",
    "# Create Prices DataFrame\n",
    "from pyspark.sql.functions import to_date\n",
    "prices_df = spark.createDataFrame(prices_data, [\"product_id\", \"start_date\", \"end_date\", \"price\"]) \\\n",
    "    .withColumn(\"start_date\", to_date(\"start_date\")) \\\n",
    "    .withColumn(\"end_date\", to_date(\"end_date\"))\n",
    "\n",
    "# Define schema for UnitsSold\n",
    "units_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"purchase_date\", DateType(), True),\n",
    "    StructField(\"units\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Define data for UnitsSold\n",
    "units_data = [\n",
    "    (1, \"2019-02-25\", 100),\n",
    "    (1, \"2019-03-01\", 15),\n",
    "    (2, \"2019-02-10\", 200),\n",
    "    (2, \"2019-03-22\", 30)\n",
    "]\n",
    "\n",
    "# Create UnitsSold DataFrame\n",
    "units_df = spark.createDataFrame(units_data, [\"product_id\", \"purchase_date\", \"units\"]) \\\n",
    "    .withColumn(\"purchase_date\", to_date(\"purchase_date\"))\n",
    "\n",
    "# Show both DataFrames\n",
    "print(\"Prices Table:\")\n",
    "prices_df.show()\n",
    "\n",
    "print(\"UnitsSold Table:\")\n",
    "units_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ca4c73-03d1-41c3-9ea3-0e74113fe348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f23c700f-a730-4db8-b2a5-a1f71a88e325",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prices_df.alias(\"p\").join(\n",
    "    units_df.alias(\"u\"),\n",
    "    (col(\"p.product_id\") == col(\"u.product_id\"))\n",
    "    & (col(\"u.purchase_date\").between(col(\"p.start_date\"), col(\"p.end_date\"))),\n",
    "    \"left\",\n",
    ").selectExpr(\"p.product_id\", \"u.units\", \"p.price * u.units as amount\").fillna(\n",
    "    {\"units\": 0}\n",
    ").groupBy(\"product_id\").agg(round((sum(\"amount\")/sum(\"units\")),2).alias(\"average_price\")).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f21c43d-6650-4443-9337-14c4ccac6ebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 17**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26af0c22-578b-48b1-a944-3f75b073a8c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Define schema for Project table\n",
    "project_schema = StructType([\n",
    "    StructField(\"project_id\", IntegerType(), True),\n",
    "    StructField(\"employee_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Data for Project table\n",
    "project_data = [\n",
    "    (1, 1),\n",
    "    (1, 2),\n",
    "    (1, 3),\n",
    "    (2, 1),\n",
    "    (2, 4)\n",
    "]\n",
    "\n",
    "# Create Project DataFrame\n",
    "project_df = spark.createDataFrame(project_data, schema=project_schema)\n",
    "\n",
    "# Define schema for Employee table\n",
    "employee_schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"experience_years\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Data for Employee table\n",
    "employee_data = [\n",
    "    (1, \"Khaled\", 3),\n",
    "    (2, \"Ali\", 2),\n",
    "    (3, \"John\", 1),\n",
    "    (4, \"Doe\", 2)\n",
    "]\n",
    "\n",
    "# Create Employee DataFrame\n",
    "employee_df = spark.createDataFrame(employee_data, schema=employee_schema)\n",
    "\n",
    "# Show DataFrames\n",
    "print(\"Project Table:\")\n",
    "project_df.show()\n",
    "\n",
    "print(\"Employee Table:\")\n",
    "employee_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02e47843-5d47-4f8c-8cae-3e6bda047d6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f9900d5-3a7c-48b8-95cd-8a23c6523ac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "project_df.alias(\"p\").join(employee_df.alias('e'),col('p.employee_id') == col('e.employee_id'),\"inner\").groupBy(\"project_id\").agg(round(avg(\"experience_years\"),2).alias(\"average_years\")).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9348e07f-5218-404a-8e41-236b90e314c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 18**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "304e3d5b-12a6-46db-a96e-0f446671e3bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "\n",
    "# Define schema for Users table\n",
    "users_schema = StructType([\n",
    "    StructField(\"user_id\", IntegerType(), True),\n",
    "    StructField(\"user_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Data for Users table\n",
    "users_data = [\n",
    "    (6, \"Alice\"),\n",
    "    (2, \"Bob\"),\n",
    "    (7, \"Alex\")\n",
    "]\n",
    "\n",
    "# Create Users DataFrame\n",
    "users_df = spark.createDataFrame(users_data, schema=users_schema)\n",
    "\n",
    "# Define schema for Register table\n",
    "register_schema = StructType([\n",
    "    StructField(\"contest_id\", IntegerType(), True),\n",
    "    StructField(\"user_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Data for Register table\n",
    "register_data = [\n",
    "    (215, 6),\n",
    "    (209, 2),\n",
    "    (208, 2),\n",
    "    (210, 6),\n",
    "    (208, 6),\n",
    "    (209, 7),\n",
    "    (209, 6),\n",
    "    (215, 7),\n",
    "    (208, 7),\n",
    "    (210, 2),\n",
    "    (207, 2),\n",
    "    (210, 7)\n",
    "]\n",
    "\n",
    "# Create Register DataFrame\n",
    "register_df = spark.createDataFrame(register_data, schema=register_schema)\n",
    "\n",
    "# Show DataFrames\n",
    "print(\"Users Table:\")\n",
    "users_df.show()\n",
    "\n",
    "print(\"Register Table:\")\n",
    "register_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c2f35f1-3e0e-422e-b7e2-9f1ac9a0ccf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "register_df.groupBy(\"contest_id\").agg(\n",
    "    round(((count(\"user_id\") / users_df.count()) * 100.0), 2).alias(\"percentage\")\n",
    ").orderBy(desc(\"percentage\"), \"contest_id\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SQL 50",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
