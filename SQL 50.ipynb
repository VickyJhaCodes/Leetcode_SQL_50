{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f64b8bf9-5bcb-4c44-b901-307c8fdc9570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**SQL 50 Link: https://leetcode.com/studyplan/top-sql-50/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9734b5b-42c7-41f5-8f76-d9a80b1645db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8224caf7-e491-4056-8687-a68f56869c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"low_fats\", StringType(), True),\n",
    "    StructField(\"recyclable\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    (0, \"Y\", \"N\"),\n",
    "    (1, \"Y\", \"Y\"),\n",
    "    (2, \"N\", \"Y\"),\n",
    "    (3, \"Y\", \"Y\"),\n",
    "    (4, \"N\", \"N\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce8de5eb-72c7-4956-83d2-6447da8da3d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a727068b-c155-4bd9-b0dc-a5bd04bbbac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter((col(\"low_fats\") == \"Y\") & (col(\"recyclable\") == \"Y\")).select(\n",
    "    \"product_id\"\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b30f8cf-66ef-4b4a-bb5a-447374293742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05673002-0ebe-40e9-bc13-7caab4e280d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"referee_id\", IntegerType(), True) \n",
    "])\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    (1, \"Will\", None),\n",
    "    (2, \"Jane\", None),\n",
    "    (3, \"Alex\", 2),\n",
    "    (4, \"Bill\", None),\n",
    "    (5, \"Zack\", 1),\n",
    "    (6, \"Mark\", 2)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92251def-0de8-40a5-8bd1-9cdced6a1e4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c118c79-1f6f-40d2-97e0-07005e047b01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter((col('referee_id') != 2) | (col('referee_id').isNull())).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a287f27-a040-44f2-9008-721392436814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61fcac66-e3d2-4990-a518-64d69f05384c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"continent\", StringType(), True),\n",
    "    StructField(\"area\", LongType(), True),\n",
    "    StructField(\"population\", LongType(), True),\n",
    "    StructField(\"gdp\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    (\"Afghanistan\", \"Asia\", 652230, 25500100, 20343000000),\n",
    "    (\"Albania\", \"Europe\", 28748, 2831741, 12960000000),\n",
    "    (\"Algeria\", \"Africa\", 2381741, 37100000, 188681000000),\n",
    "    (\"Andorra\", \"Europe\", 468, 78115, 3712000000),\n",
    "    (\"Angola\", \"Africa\", 1246700, 20609294, 100990000000)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15ccaad8-f27e-4ff4-9c90-73529c59e573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d400aab-b108-4dfc-9c76-77e487332f18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter((col('area')>=3000000) | (col('population')>=25000000)).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "156ad50e-01bc-47e4-995f-1f64b11fb796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98ef0575-2093-4ef4-be14-db41d91c392b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DateType\n",
    "from datetime import datetime\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"article_id\", IntegerType(), True),\n",
    "    StructField(\"author_id\", IntegerType(), True),\n",
    "    StructField(\"viewer_id\", IntegerType(), True),\n",
    "    StructField(\"view_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    (1, 3, 5, datetime.strptime(\"2019-08-01\", \"%Y-%m-%d\").date()),\n",
    "    (1, 3, 6, datetime.strptime(\"2019-08-02\", \"%Y-%m-%d\").date()),\n",
    "    (2, 7, 7, datetime.strptime(\"2019-08-01\", \"%Y-%m-%d\").date()),\n",
    "    (2, 7, 6, datetime.strptime(\"2019-08-02\", \"%Y-%m-%d\").date()),\n",
    "    (4, 7, 1, datetime.strptime(\"2019-07-22\", \"%Y-%m-%d\").date()),\n",
    "    (3, 4, 4, datetime.strptime(\"2019-07-21\", \"%Y-%m-%d\").date()),\n",
    "    (3, 4, 4, datetime.strptime(\"2019-07-21\", \"%Y-%m-%d\").date())\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show DataFrame\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5edb62d-15be-4f39-94e9-467346bd7927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7b9a36-8a59-4750-99bb-c57871bb991a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.filter((col('author_id') == col('viewer_id'))).select(col('author_id').alias(\"id\")).distinct().orderBy(asc(\"id\")).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71c61fd7-f96c-4185-827e-bb3ad364de3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70acbd7c-a143-4e7d-bf05-8b6a63ce1429",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"tweet_id\", IntegerType(), True),\n",
    "    StructField(\"content\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    (1, \"Let us Code\"),\n",
    "    (2, \"More than fifteen chars are here!\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73d995eb-410e-4bd6-a9d3-32489e41245f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "457d9de5-0bfc-4b65-a91d-a602c0db5676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length\n",
    "df.filter((length(col('content'))>15)).select(\"tweet_id\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c01cf1a9-4c06-4077-8c8e-6382038e3c43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66ef3524-0e2e-4fdd-906d-b8c2191c7efa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# --- Employees Table ---\n",
    "\n",
    "# Define schema for Employees\n",
    "employees_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Employees data\n",
    "employees_data = [\n",
    "    (1, \"Alice\"),\n",
    "    (7, \"Bob\"),\n",
    "    (11, \"Meir\"),\n",
    "    (90, \"Winston\"),\n",
    "    (3, \"Jonathan\")\n",
    "]\n",
    "\n",
    "# Create Employees DataFrame\n",
    "employees_df = spark.createDataFrame(employees_data, employees_schema)\n",
    "\n",
    "# --- EmployeeUNI Table ---\n",
    "\n",
    "# Define schema for EmployeeUNI\n",
    "employee_uni_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"unique_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# EmployeeUNI data\n",
    "employee_uni_data = [\n",
    "    (3, 1),\n",
    "    (11, 2),\n",
    "    (90, 3)\n",
    "]\n",
    "\n",
    "# Create EmployeeUNI DataFrame\n",
    "employee_uni_df = spark.createDataFrame(employee_uni_data, employee_uni_schema)\n",
    "\n",
    "# Show both DataFrames\n",
    "print(\"Employees DataFrame:\")\n",
    "employees_df.show()\n",
    "\n",
    "print(\"EmployeeUNI DataFrame:\")\n",
    "employee_uni_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d88b05a0-df15-4448-9c7e-ce814f2f9ab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39fe26b3-1714-4a14-9a3e-32ebc68df1bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *  #a\n",
    "df= employees_df.alias('e').join(employee_uni_df.alias(\"eu\"),col('e.id')==col('eu.id'),\"left\").select(\"eu.unique_id\",\"e.name\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98aabf67-487b-4a59-8292-e9b57ecfec90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de581601-229a-4265-a699-3e4e82ee3604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# --- Sales Table ---\n",
    "\n",
    "# Define schema for Sales\n",
    "sales_schema = StructType([\n",
    "    StructField(\"sale_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sales data\n",
    "sales_data = [\n",
    "    (1, 100, 2008, 10, 5000),\n",
    "    (2, 100, 2009, 12, 5000),\n",
    "    (7, 200, 2011, 15, 9000)\n",
    "]\n",
    "\n",
    "# Create Sales DataFrame\n",
    "sales_df = spark.createDataFrame(sales_data, sales_schema)\n",
    "\n",
    "# --- Product Table ---\n",
    "\n",
    "# Define schema for Product\n",
    "product_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"product_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Product data\n",
    "product_data = [\n",
    "    (100, \"Nokia\"),\n",
    "    (200, \"Apple\"),\n",
    "    (300, \"Samsung\")\n",
    "]\n",
    "\n",
    "# Create Product DataFrame\n",
    "product_df = spark.createDataFrame(product_data, product_schema)\n",
    "\n",
    "# Show both DataFrames\n",
    "print(\"Sales DataFrame:\")\n",
    "sales_df.show()\n",
    "\n",
    "print(\"Product DataFrame:\")\n",
    "product_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c5bc74b-bd0f-4af7-89e4-1c23f030fe5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9c5ff57-66db-4360-9421-1bb0342e799d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_df.alias(\"s\").join(product_df.alias(\"p\"),col('s.product_id') == col('p.product_id'),\"inner\").select(\"p.product_name\",\"s.year\",\"s.price\").display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0fbf5f0-3d21-4e6c-ba42-b76cf19aa118",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "112072df-53e1-4562-ae2a-c553caa7e6b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SQL 50",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
