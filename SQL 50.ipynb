{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f64b8bf9-5bcb-4c44-b901-307c8fdc9570",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**SQL 50 Link: https://leetcode.com/studyplan/top-sql-50/**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9734b5b-42c7-41f5-8f76-d9a80b1645db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8224caf7-e491-4056-8687-a68f56869c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"low_fats\", StringType(), True),\n",
    "    StructField(\"recyclable\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    (0, \"Y\", \"N\"),\n",
    "    (1, \"Y\", \"Y\"),\n",
    "    (2, \"N\", \"Y\"),\n",
    "    (3, \"Y\", \"Y\"),\n",
    "    (4, \"N\", \"N\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce8de5eb-72c7-4956-83d2-6447da8da3d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a727068b-c155-4bd9-b0dc-a5bd04bbbac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter((col(\"low_fats\") == \"Y\") & (col(\"recyclable\") == \"Y\")).select(\n",
    "    \"product_id\"\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b30f8cf-66ef-4b4a-bb5a-447374293742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05673002-0ebe-40e9-bc13-7caab4e280d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"referee_id\", IntegerType(), True) \n",
    "])\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    (1, \"Will\", None),\n",
    "    (2, \"Jane\", None),\n",
    "    (3, \"Alex\", 2),\n",
    "    (4, \"Bill\", None),\n",
    "    (5, \"Zack\", 1),\n",
    "    (6, \"Mark\", 2)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92251def-0de8-40a5-8bd1-9cdced6a1e4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c118c79-1f6f-40d2-97e0-07005e047b01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter((col('referee_id') != 2) | (col('referee_id').isNull())).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a287f27-a040-44f2-9008-721392436814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61fcac66-e3d2-4990-a518-64d69f05384c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"continent\", StringType(), True),\n",
    "    StructField(\"area\", LongType(), True),\n",
    "    StructField(\"population\", LongType(), True),\n",
    "    StructField(\"gdp\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    (\"Afghanistan\", \"Asia\", 652230, 25500100, 20343000000),\n",
    "    (\"Albania\", \"Europe\", 28748, 2831741, 12960000000),\n",
    "    (\"Algeria\", \"Africa\", 2381741, 37100000, 188681000000),\n",
    "    (\"Andorra\", \"Europe\", 468, 78115, 3712000000),\n",
    "    (\"Angola\", \"Africa\", 1246700, 20609294, 100990000000)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15ccaad8-f27e-4ff4-9c90-73529c59e573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d400aab-b108-4dfc-9c76-77e487332f18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter((col('area')>=3000000) | (col('population')>=25000000)).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "156ad50e-01bc-47e4-995f-1f64b11fb796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98ef0575-2093-4ef4-be14-db41d91c392b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DateType\n",
    "from datetime import datetime\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"article_id\", IntegerType(), True),\n",
    "    StructField(\"author_id\", IntegerType(), True),\n",
    "    StructField(\"viewer_id\", IntegerType(), True),\n",
    "    StructField(\"view_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    (1, 3, 5, datetime.strptime(\"2019-08-01\", \"%Y-%m-%d\").date()),\n",
    "    (1, 3, 6, datetime.strptime(\"2019-08-02\", \"%Y-%m-%d\").date()),\n",
    "    (2, 7, 7, datetime.strptime(\"2019-08-01\", \"%Y-%m-%d\").date()),\n",
    "    (2, 7, 6, datetime.strptime(\"2019-08-02\", \"%Y-%m-%d\").date()),\n",
    "    (4, 7, 1, datetime.strptime(\"2019-07-22\", \"%Y-%m-%d\").date()),\n",
    "    (3, 4, 4, datetime.strptime(\"2019-07-21\", \"%Y-%m-%d\").date()),\n",
    "    (3, 4, 4, datetime.strptime(\"2019-07-21\", \"%Y-%m-%d\").date())\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show DataFrame\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5edb62d-15be-4f39-94e9-467346bd7927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f7b9a36-8a59-4750-99bb-c57871bb991a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.filter((col('author_id') == col('viewer_id'))).select(col('author_id').alias(\"id\")).distinct().orderBy(asc(\"id\")).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71c61fd7-f96c-4185-827e-bb3ad364de3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70acbd7c-a143-4e7d-bf05-8b6a63ce1429",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"tweet_id\", IntegerType(), True),\n",
    "    StructField(\"content\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Data\n",
    "data = [\n",
    "    (1, \"Let us Code\"),\n",
    "    (2, \"More than fifteen chars are here!\")\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Show DataFrame\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73d995eb-410e-4bd6-a9d3-32489e41245f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "457d9de5-0bfc-4b65-a91d-a602c0db5676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length\n",
    "df.filter((length(col('content'))>15)).select(\"tweet_id\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c01cf1a9-4c06-4077-8c8e-6382038e3c43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66ef3524-0e2e-4fdd-906d-b8c2191c7efa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# --- Employees Table ---\n",
    "\n",
    "# Define schema for Employees\n",
    "employees_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Employees data\n",
    "employees_data = [\n",
    "    (1, \"Alice\"),\n",
    "    (7, \"Bob\"),\n",
    "    (11, \"Meir\"),\n",
    "    (90, \"Winston\"),\n",
    "    (3, \"Jonathan\")\n",
    "]\n",
    "\n",
    "# Create Employees DataFrame\n",
    "employees_df = spark.createDataFrame(employees_data, employees_schema)\n",
    "\n",
    "# --- EmployeeUNI Table ---\n",
    "\n",
    "# Define schema for EmployeeUNI\n",
    "employee_uni_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"unique_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# EmployeeUNI data\n",
    "employee_uni_data = [\n",
    "    (3, 1),\n",
    "    (11, 2),\n",
    "    (90, 3)\n",
    "]\n",
    "\n",
    "# Create EmployeeUNI DataFrame\n",
    "employee_uni_df = spark.createDataFrame(employee_uni_data, employee_uni_schema)\n",
    "\n",
    "# Show both DataFrames\n",
    "print(\"Employees DataFrame:\")\n",
    "employees_df.show()\n",
    "\n",
    "print(\"EmployeeUNI DataFrame:\")\n",
    "employee_uni_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d88b05a0-df15-4448-9c7e-ce814f2f9ab3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39fe26b3-1714-4a14-9a3e-32ebc68df1bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *  #a\n",
    "df= employees_df.alias('e').join(employee_uni_df.alias(\"eu\"),col('e.id')==col('eu.id'),\"left\").select(\"eu.unique_id\",\"e.name\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98aabf67-487b-4a59-8292-e9b57ecfec90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de581601-229a-4265-a699-3e4e82ee3604",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# --- Sales Table ---\n",
    "\n",
    "# Define schema for Sales\n",
    "sales_schema = StructType([\n",
    "    StructField(\"sale_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"year\", IntegerType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Sales data\n",
    "sales_data = [\n",
    "    (1, 100, 2008, 10, 5000),\n",
    "    (2, 100, 2009, 12, 5000),\n",
    "    (7, 200, 2011, 15, 9000)\n",
    "]\n",
    "\n",
    "# Create Sales DataFrame\n",
    "sales_df = spark.createDataFrame(sales_data, sales_schema)\n",
    "\n",
    "# --- Product Table ---\n",
    "\n",
    "# Define schema for Product\n",
    "product_schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"product_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Product data\n",
    "product_data = [\n",
    "    (100, \"Nokia\"),\n",
    "    (200, \"Apple\"),\n",
    "    (300, \"Samsung\")\n",
    "]\n",
    "\n",
    "# Create Product DataFrame\n",
    "product_df = spark.createDataFrame(product_data, product_schema)\n",
    "\n",
    "# Show both DataFrames\n",
    "print(\"Sales DataFrame:\")\n",
    "sales_df.show()\n",
    "\n",
    "print(\"Product DataFrame:\")\n",
    "product_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c5bc74b-bd0f-4af7-89e4-1c23f030fe5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9c5ff57-66db-4360-9421-1bb0342e799d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sales_df.alias(\"s\").join(product_df.alias(\"p\"),col('s.product_id') == col('p.product_id'),\"inner\").select(\"p.product_name\",\"s.year\",\"s.price\").display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0fbf5f0-3d21-4e6c-ba42-b76cf19aa118",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "112072df-53e1-4562-ae2a-c553caa7e6b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "\n",
    "\n",
    "# --- Visits Table ---\n",
    "\n",
    "# Define schema for Visits\n",
    "visits_schema = StructType([\n",
    "    StructField(\"visit_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Visits data\n",
    "visits_data = [\n",
    "    (1, 23),\n",
    "    (2, 9),\n",
    "    (4, 30),\n",
    "    (5, 54),\n",
    "    (6, 96),\n",
    "    (7, 54),\n",
    "    (8, 54)\n",
    "]\n",
    "\n",
    "# Create Visits DataFrame\n",
    "visits_df = spark.createDataFrame(visits_data, visits_schema)\n",
    "\n",
    "# --- Transactions Table ---\n",
    "\n",
    "# Define schema for Transactions\n",
    "transactions_schema = StructType([\n",
    "    StructField(\"transaction_id\", IntegerType(), True),\n",
    "    StructField(\"visit_id\", IntegerType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Transactions data\n",
    "transactions_data = [\n",
    "    (2, 5, 310),\n",
    "    (3, 5, 300),\n",
    "    (9, 5, 200),\n",
    "    (12, 1, 910),\n",
    "    (13, 2, 970)\n",
    "]\n",
    "\n",
    "# Create Transactions DataFrame\n",
    "transactions_df = spark.createDataFrame(transactions_data, transactions_schema)\n",
    "\n",
    "# Show both DataFrames\n",
    "print(\"Visits DataFrame:\")\n",
    "visits_df.show()\n",
    "\n",
    "print(\"Transactions DataFrame:\")\n",
    "transactions_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4c4fe7c-de84-4673-9126-51660ba766ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "385dda6e-cafb-4f80-b65c-fedc194384dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "visits_df.alias(\"v\").join(\n",
    "    transactions_df.alias(\"t\"), col('v.visit_id') == col('t.visit_id'), \"left\"\n",
    ").filter(col(\"t.transaction_id\").isNull()).groupBy(col('v.customer_id')).agg(count(\"*\").alias(\"count_no_trans\")).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81edd879-ff7d-4c92-90ca-aafd0a9989ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b62b5df5-7846-452a-befe-ad68f2e5b247",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DateType\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Define schema for Weather\n",
    "weather_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"recordDate\", DateType(), True),\n",
    "    StructField(\"temperature\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Weather data\n",
    "weather_data = [\n",
    "    (1, datetime.strptime(\"2015-01-01\", \"%Y-%m-%d\").date(), 10),\n",
    "    (2, datetime.strptime(\"2015-01-02\", \"%Y-%m-%d\").date(), 25),\n",
    "    (3, datetime.strptime(\"2015-01-03\", \"%Y-%m-%d\").date(), 20),\n",
    "    (4, datetime.strptime(\"2015-01-04\", \"%Y-%m-%d\").date(), 30)\n",
    "]\n",
    "\n",
    "# Create Weather DataFrame\n",
    "weather_df = spark.createDataFrame(weather_data, weather_schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "weather_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eff4103b-51c9-4e05-9d83-36fc60fdbbab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 9**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1feac3c-f66c-4f7a-8424-bb9f55bf7706",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1752765591938}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "myWindow = Window.orderBy(\"recordDate\")\n",
    "weather_df.withColumn(\"prev_day\", lag(col(\"recordDate\"), 1).over(myWindow)).withColumn(\n",
    "    \"prev_day_temp\", lag(col(\"temperature\"), 1).over(myWindow)\n",
    ").filter(\n",
    "    (abs(date_diff(col(\"recordDate\"), col(\"prev_day\"))) == 1)\n",
    "    & (col(\"temperature\") > col(\"prev_day_temp\"))\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36fe22ba-fab0-475a-bc3d-f8ab13cb00b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Problem 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ce9e6cd-92d8-4a6d-853b-30698001193d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "# Define schema for Activity\n",
    "activity_schema = StructType([\n",
    "    StructField(\"machine_id\", IntegerType(), True),\n",
    "    StructField(\"process_id\", IntegerType(), True),\n",
    "    StructField(\"activity_type\", StringType(), True),\n",
    "    StructField(\"timestamp\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Activity data\n",
    "activity_data = [\n",
    "    (0, 0, \"start\", 0.712),\n",
    "    (0, 0, \"end\", 1.520),\n",
    "    (0, 1, \"start\", 3.140),\n",
    "    (0, 1, \"end\", 4.120),\n",
    "    (1, 0, \"start\", 0.550),\n",
    "    (1, 0, \"end\", 1.550),\n",
    "    (1, 1, \"start\", 0.430),\n",
    "    (1, 1, \"end\", 1.420),\n",
    "    (2, 0, \"start\", 4.100),\n",
    "    (2, 0, \"end\", 4.512),\n",
    "    (2, 1, \"start\", 2.500),\n",
    "    (2, 1, \"end\", 5.000)\n",
    "]\n",
    "\n",
    "# Create Activity DataFrame\n",
    "activity_df = spark.createDataFrame(activity_data, activity_schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "activity_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10d8e1d7-0f54-4beb-9359-a42f8eedf2a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45c2adf9-bac1-4d67-9730-1b3b555b1e33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "activity_df.alias(\"a1\").join(\n",
    "    activity_df.alias(\"a2\"),\n",
    "    (\n",
    "        (col(\"a1.machine_id\") == col(\"a2.machine_id\"))\n",
    "        & (col(\"a1.process_id\") == col(\"a2.process_id\"))\n",
    "        & (col(\"a2.timestamp\") > col(\"a1.timestamp\"))\n",
    "    ),\n",
    "    \"inner\",\n",
    ").selectExpr(\n",
    "    \"a1.machine_id\", \"a1.process_id\", \"a2.timestamp - a1.timestamp as processing_time\"\n",
    ").groupBy(\"a1.machine_id\").agg((sum(col('processing_time'))/count(col('process_id'))).alias(\"avg_processing\")).display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SQL 50",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
